{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2c2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.dataloader import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2670756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANCER HISTOPATHOLOGY CNN CLASSIFIER\n",
      "\n",
      " Running quick CNN connection test...\n",
      "\n",
      " Starting actual training...\n",
      "Traisn MODEL\n",
      "\n",
      "[SETUP] Device: cuda\n",
      "[SETUP] GPU: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "[SETUP] CUDA Memory: 8.6 GB\n",
      "\n",
      "[DATA] Loading dataset...\n",
      "âœ“ Root directory: lungcolon\n",
      "  colon_aca: 5000 images\n",
      "  colon_n: 5000 images\n",
      "  lung_aca: 5000 images\n",
      "  lung_n: 5000 images\n",
      "  lung_scc: 5000 images\n",
      "Total samples loaded: 25000\n",
      "\n",
      "[DATA] Dataset splits:\n",
      "  Training: 17500 samples\n",
      "  Validation: 3750 samples\n",
      "  Test: 3750 samples\n",
      "  Classes: ['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc']\n",
      "MODEL ARCHITECTURE\n",
      "CancerCNN(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=14400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "[MODEL] Total parameters: 1,741,137\n",
      "[MODEL] Trainable parameters: 1,741,137\n",
      "TRAINING\n",
      "\n",
      "[EPOCH 1/5] Starting...\n",
      "[BATCH 1] Loss: 1.6172, Acc: 9.4%, Time: 0.4s\n",
      "[INFO] First batch completed! Training continues...\n",
      "[BATCH 10] Avg Loss: 1.5458, Current Acc: 34.4%\n",
      "[BATCH 20] Avg Loss: 1.2426, Current Acc: 46.9%\n",
      "[BATCH 30] Avg Loss: 1.1047, Current Acc: 52.7%\n",
      "[BATCH 40] Avg Loss: 1.0278, Current Acc: 55.3%\n",
      "[BATCH 50] Avg Loss: 0.9627, Current Acc: 57.0%\n",
      "[BATCH 60] Avg Loss: 0.9116, Current Acc: 58.5%\n",
      "[BATCH 70] Avg Loss: 0.8798, Current Acc: 59.5%\n",
      "[BATCH 80] Avg Loss: 0.8426, Current Acc: 60.4%\n",
      "[BATCH 90] Avg Loss: 0.8163, Current Acc: 61.5%\n",
      "[BATCH 100] Avg Loss: 0.7933, Current Acc: 62.2%\n",
      "[BATCH 110] Avg Loss: 0.7672, Current Acc: 63.7%\n",
      "[BATCH 120] Avg Loss: 0.7531, Current Acc: 64.5%\n",
      "[BATCH 130] Avg Loss: 0.7396, Current Acc: 65.2%\n",
      "[BATCH 140] Avg Loss: 0.7274, Current Acc: 65.6%\n",
      "[BATCH 150] Avg Loss: 0.7220, Current Acc: 65.5%\n",
      "[BATCH 160] Avg Loss: 0.7142, Current Acc: 65.8%\n",
      "[BATCH 170] Avg Loss: 0.7051, Current Acc: 66.2%\n",
      "[BATCH 180] Avg Loss: 0.7008, Current Acc: 66.4%\n",
      "[BATCH 190] Avg Loss: 0.6970, Current Acc: 66.5%\n",
      "[BATCH 200] Avg Loss: 0.6867, Current Acc: 67.3%\n",
      "[BATCH 210] Avg Loss: 0.6770, Current Acc: 67.9%\n",
      "[BATCH 220] Avg Loss: 0.6679, Current Acc: 68.3%\n",
      "[BATCH 230] Avg Loss: 0.6580, Current Acc: 68.9%\n",
      "[BATCH 240] Avg Loss: 0.6744, Current Acc: 68.5%\n",
      "[BATCH 250] Avg Loss: 0.6773, Current Acc: 68.0%\n",
      "[BATCH 260] Avg Loss: 0.6792, Current Acc: 67.8%\n",
      "[BATCH 270] Avg Loss: 0.6770, Current Acc: 67.7%\n",
      "[BATCH 280] Avg Loss: 0.6748, Current Acc: 67.6%\n",
      "[BATCH 290] Avg Loss: 0.6707, Current Acc: 67.9%\n",
      "[BATCH 300] Avg Loss: 0.6655, Current Acc: 68.1%\n",
      "[BATCH 310] Avg Loss: 0.6593, Current Acc: 68.5%\n",
      "[BATCH 320] Avg Loss: 0.6562, Current Acc: 68.7%\n",
      "[BATCH 330] Avg Loss: 0.6497, Current Acc: 69.0%\n",
      "[BATCH 340] Avg Loss: 0.6427, Current Acc: 69.5%\n",
      "[BATCH 350] Avg Loss: 0.6380, Current Acc: 69.8%\n",
      "[BATCH 360] Avg Loss: 0.6314, Current Acc: 70.2%\n",
      "[BATCH 370] Avg Loss: 0.6262, Current Acc: 70.4%\n",
      "[BATCH 380] Avg Loss: 0.6226, Current Acc: 70.7%\n",
      "[BATCH 390] Avg Loss: 0.6216, Current Acc: 70.6%\n",
      "[BATCH 400] Avg Loss: 0.6175, Current Acc: 70.9%\n",
      "[BATCH 410] Avg Loss: 0.6128, Current Acc: 71.2%\n",
      "[BATCH 420] Avg Loss: 0.6081, Current Acc: 71.4%\n",
      "[BATCH 430] Avg Loss: 0.6057, Current Acc: 71.5%\n",
      "[BATCH 440] Avg Loss: 0.6016, Current Acc: 71.7%\n",
      "[BATCH 450] Avg Loss: 0.5982, Current Acc: 72.0%\n",
      "[BATCH 460] Avg Loss: 0.5935, Current Acc: 72.2%\n",
      "[BATCH 470] Avg Loss: 0.5888, Current Acc: 72.5%\n",
      "[BATCH 480] Avg Loss: 0.5831, Current Acc: 72.8%\n",
      "[BATCH 490] Avg Loss: 0.5812, Current Acc: 72.9%\n",
      "[BATCH 500] Avg Loss: 0.5781, Current Acc: 73.1%\n",
      "[BATCH 510] Avg Loss: 0.5742, Current Acc: 73.3%\n",
      "[BATCH 520] Avg Loss: 0.5722, Current Acc: 73.4%\n",
      "[BATCH 530] Avg Loss: 0.5688, Current Acc: 73.5%\n",
      "[BATCH 540] Avg Loss: 0.5657, Current Acc: 73.7%\n",
      "\n",
      "[EPOCH 1 SUMMARY]\n",
      "  Loss: 0.5629\n",
      "  Accuracy: 73.87%\n",
      "  Time: 676.7 seconds\n",
      "[VALIDATION] Loss: 0.4319, Accuracy: 83.41%\n",
      "\n",
      "[EPOCH 2/5] Starting...\n",
      "[BATCH 1] Loss: 0.3624, Acc: 87.5%, Time: 0.0s\n",
      "[INFO] First batch completed! Training continues...\n",
      "[BATCH 10] Avg Loss: 0.3823, Current Acc: 84.4%\n",
      "[BATCH 20] Avg Loss: 0.4030, Current Acc: 83.1%\n",
      "[BATCH 30] Avg Loss: 0.3755, Current Acc: 84.8%\n",
      "[BATCH 40] Avg Loss: 0.3774, Current Acc: 84.8%\n",
      "[BATCH 50] Avg Loss: 0.3791, Current Acc: 84.6%\n",
      "[BATCH 60] Avg Loss: 0.3669, Current Acc: 85.2%\n",
      "[BATCH 70] Avg Loss: 0.3610, Current Acc: 85.4%\n",
      "[BATCH 80] Avg Loss: 0.3564, Current Acc: 85.8%\n",
      "[BATCH 90] Avg Loss: 0.3466, Current Acc: 86.1%\n",
      "[BATCH 100] Avg Loss: 0.3437, Current Acc: 86.1%\n",
      "[BATCH 110] Avg Loss: 0.3409, Current Acc: 86.2%\n",
      "[BATCH 120] Avg Loss: 0.3396, Current Acc: 86.3%\n",
      "[BATCH 130] Avg Loss: 0.3390, Current Acc: 86.2%\n",
      "[BATCH 140] Avg Loss: 0.3372, Current Acc: 86.3%\n",
      "[BATCH 150] Avg Loss: 0.3327, Current Acc: 86.3%\n",
      "[BATCH 160] Avg Loss: 0.3291, Current Acc: 86.4%\n",
      "[BATCH 170] Avg Loss: 0.3300, Current Acc: 86.4%\n",
      "[BATCH 180] Avg Loss: 0.3294, Current Acc: 86.5%\n",
      "[BATCH 190] Avg Loss: 0.3287, Current Acc: 86.4%\n",
      "[BATCH 200] Avg Loss: 0.3254, Current Acc: 86.6%\n",
      "[BATCH 210] Avg Loss: 0.3290, Current Acc: 86.4%\n",
      "[BATCH 220] Avg Loss: 0.3327, Current Acc: 86.3%\n",
      "[BATCH 230] Avg Loss: 0.3362, Current Acc: 86.2%\n",
      "[BATCH 240] Avg Loss: 0.3406, Current Acc: 86.1%\n",
      "[BATCH 250] Avg Loss: 0.3425, Current Acc: 86.0%\n",
      "[BATCH 260] Avg Loss: 0.3408, Current Acc: 86.0%\n",
      "[BATCH 270] Avg Loss: 0.3414, Current Acc: 86.0%\n",
      "[BATCH 280] Avg Loss: 0.3404, Current Acc: 86.1%\n",
      "[BATCH 290] Avg Loss: 0.3377, Current Acc: 86.3%\n",
      "[BATCH 300] Avg Loss: 0.3370, Current Acc: 86.3%\n",
      "[BATCH 310] Avg Loss: 0.3384, Current Acc: 86.2%\n",
      "[BATCH 320] Avg Loss: 0.3374, Current Acc: 86.3%\n",
      "[BATCH 330] Avg Loss: 0.3370, Current Acc: 86.3%\n",
      "[BATCH 340] Avg Loss: 0.3364, Current Acc: 86.3%\n",
      "[BATCH 350] Avg Loss: 0.3364, Current Acc: 86.2%\n",
      "[BATCH 360] Avg Loss: 0.3381, Current Acc: 86.1%\n",
      "[BATCH 370] Avg Loss: 0.3395, Current Acc: 86.1%\n",
      "[BATCH 380] Avg Loss: 0.3393, Current Acc: 86.1%\n",
      "[BATCH 390] Avg Loss: 0.3382, Current Acc: 86.1%\n",
      "[BATCH 400] Avg Loss: 0.3369, Current Acc: 86.1%\n",
      "[BATCH 410] Avg Loss: 0.3370, Current Acc: 86.1%\n",
      "[BATCH 420] Avg Loss: 0.3362, Current Acc: 86.2%\n",
      "[BATCH 430] Avg Loss: 0.3357, Current Acc: 86.2%\n",
      "[BATCH 440] Avg Loss: 0.3376, Current Acc: 86.0%\n",
      "[BATCH 450] Avg Loss: 0.3379, Current Acc: 86.0%\n",
      "[BATCH 460] Avg Loss: 0.3394, Current Acc: 85.9%\n",
      "[BATCH 470] Avg Loss: 0.3393, Current Acc: 85.9%\n",
      "[BATCH 480] Avg Loss: 0.3386, Current Acc: 85.9%\n",
      "[BATCH 490] Avg Loss: 0.3385, Current Acc: 85.9%\n",
      "[BATCH 500] Avg Loss: 0.3371, Current Acc: 85.9%\n",
      "[BATCH 510] Avg Loss: 0.3363, Current Acc: 85.9%\n",
      "[BATCH 520] Avg Loss: 0.3354, Current Acc: 86.0%\n",
      "[BATCH 530] Avg Loss: 0.3346, Current Acc: 86.0%\n",
      "[BATCH 540] Avg Loss: 0.3341, Current Acc: 86.0%\n",
      "\n",
      "[EPOCH 2 SUMMARY]\n",
      "  Loss: 0.3329\n",
      "  Accuracy: 86.07%\n",
      "  Time: 946.2 seconds\n",
      "[VALIDATION] Loss: 0.2636, Accuracy: 89.63%\n",
      "\n",
      "[EPOCH 3/5] Starting...\n",
      "[BATCH 1] Loss: 0.1503, Acc: 96.9%, Time: 0.0s\n",
      "[INFO] First batch completed! Training continues...\n",
      "[BATCH 10] Avg Loss: 0.1875, Current Acc: 92.8%\n",
      "[BATCH 20] Avg Loss: 0.2293, Current Acc: 90.0%\n",
      "[BATCH 30] Avg Loss: 0.2366, Current Acc: 90.0%\n",
      "[BATCH 40] Avg Loss: 0.2336, Current Acc: 91.1%\n",
      "[BATCH 50] Avg Loss: 0.2342, Current Acc: 90.9%\n",
      "[BATCH 60] Avg Loss: 0.2463, Current Acc: 90.3%\n",
      "[BATCH 70] Avg Loss: 0.2412, Current Acc: 90.4%\n",
      "[BATCH 80] Avg Loss: 0.2357, Current Acc: 90.7%\n",
      "[BATCH 90] Avg Loss: 0.2325, Current Acc: 90.8%\n",
      "[BATCH 100] Avg Loss: 0.2287, Current Acc: 90.9%\n",
      "[BATCH 110] Avg Loss: 0.2242, Current Acc: 91.2%\n",
      "[BATCH 120] Avg Loss: 0.2271, Current Acc: 91.1%\n",
      "[BATCH 130] Avg Loss: 0.2345, Current Acc: 90.7%\n",
      "[BATCH 140] Avg Loss: 0.2399, Current Acc: 90.7%\n",
      "[BATCH 150] Avg Loss: 0.2390, Current Acc: 90.8%\n",
      "[BATCH 160] Avg Loss: 0.2513, Current Acc: 90.1%\n",
      "[BATCH 170] Avg Loss: 0.2504, Current Acc: 90.2%\n",
      "[BATCH 180] Avg Loss: 0.2522, Current Acc: 90.2%\n",
      "[BATCH 190] Avg Loss: 0.2498, Current Acc: 90.3%\n",
      "[BATCH 200] Avg Loss: 0.2488, Current Acc: 90.3%\n",
      "[BATCH 210] Avg Loss: 0.2452, Current Acc: 90.4%\n",
      "[BATCH 220] Avg Loss: 0.2424, Current Acc: 90.6%\n",
      "[BATCH 230] Avg Loss: 0.2423, Current Acc: 90.6%\n",
      "[BATCH 240] Avg Loss: 0.2444, Current Acc: 90.4%\n",
      "[BATCH 250] Avg Loss: 0.2455, Current Acc: 90.4%\n",
      "[BATCH 260] Avg Loss: 0.2468, Current Acc: 90.3%\n",
      "[BATCH 270] Avg Loss: 0.2446, Current Acc: 90.4%\n",
      "[BATCH 280] Avg Loss: 0.2432, Current Acc: 90.4%\n",
      "[BATCH 290] Avg Loss: 0.2426, Current Acc: 90.4%\n",
      "[BATCH 300] Avg Loss: 0.2424, Current Acc: 90.5%\n",
      "[BATCH 310] Avg Loss: 0.2420, Current Acc: 90.5%\n",
      "[BATCH 320] Avg Loss: 0.2408, Current Acc: 90.5%\n",
      "[BATCH 330] Avg Loss: 0.2384, Current Acc: 90.6%\n",
      "[BATCH 340] Avg Loss: 0.2359, Current Acc: 90.6%\n",
      "[BATCH 350] Avg Loss: 0.2344, Current Acc: 90.7%\n",
      "[BATCH 360] Avg Loss: 0.2325, Current Acc: 90.8%\n",
      "[BATCH 370] Avg Loss: 0.2315, Current Acc: 90.8%\n",
      "[BATCH 380] Avg Loss: 0.2300, Current Acc: 90.9%\n",
      "[BATCH 390] Avg Loss: 0.2289, Current Acc: 91.0%\n",
      "[BATCH 400] Avg Loss: 0.2274, Current Acc: 91.0%\n",
      "[BATCH 410] Avg Loss: 0.2249, Current Acc: 91.1%\n",
      "[BATCH 420] Avg Loss: 0.2241, Current Acc: 91.2%\n",
      "[BATCH 430] Avg Loss: 0.2238, Current Acc: 91.2%\n",
      "[BATCH 440] Avg Loss: 0.2227, Current Acc: 91.3%\n",
      "[BATCH 450] Avg Loss: 0.2213, Current Acc: 91.4%\n",
      "[BATCH 460] Avg Loss: 0.2214, Current Acc: 91.4%\n",
      "[BATCH 470] Avg Loss: 0.2211, Current Acc: 91.4%\n",
      "[BATCH 480] Avg Loss: 0.2206, Current Acc: 91.4%\n",
      "[BATCH 490] Avg Loss: 0.2219, Current Acc: 91.4%\n",
      "[BATCH 500] Avg Loss: 0.2221, Current Acc: 91.3%\n",
      "[BATCH 510] Avg Loss: 0.2215, Current Acc: 91.3%\n",
      "[BATCH 520] Avg Loss: 0.2214, Current Acc: 91.4%\n",
      "[BATCH 530] Avg Loss: 0.2212, Current Acc: 91.4%\n",
      "[BATCH 540] Avg Loss: 0.2211, Current Acc: 91.4%\n",
      "\n",
      "[EPOCH 3 SUMMARY]\n",
      "  Loss: 0.2224\n",
      "  Accuracy: 91.31%\n",
      "  Time: 1067.8 seconds\n",
      "[VALIDATION] Loss: 0.2945, Accuracy: 87.12%\n",
      "\n",
      "[EPOCH 4/5] Starting...\n",
      "[BATCH 1] Loss: 0.1481, Acc: 93.8%, Time: 0.0s\n",
      "[INFO] First batch completed! Training continues...\n",
      "[BATCH 10] Avg Loss: 0.2731, Current Acc: 87.8%\n",
      "[BATCH 20] Avg Loss: 0.2175, Current Acc: 90.8%\n",
      "[BATCH 30] Avg Loss: 0.2290, Current Acc: 90.6%\n",
      "[BATCH 40] Avg Loss: 0.2384, Current Acc: 89.8%\n",
      "[BATCH 50] Avg Loss: 0.2553, Current Acc: 89.1%\n",
      "[BATCH 60] Avg Loss: 0.2565, Current Acc: 89.0%\n",
      "[BATCH 70] Avg Loss: 0.2425, Current Acc: 89.8%\n",
      "[BATCH 80] Avg Loss: 0.2286, Current Acc: 90.6%\n",
      "[BATCH 90] Avg Loss: 0.2242, Current Acc: 90.6%\n",
      "[BATCH 100] Avg Loss: 0.2155, Current Acc: 90.9%\n",
      "[BATCH 110] Avg Loss: 0.2107, Current Acc: 91.1%\n",
      "[BATCH 120] Avg Loss: 0.2143, Current Acc: 91.2%\n",
      "[BATCH 130] Avg Loss: 0.2109, Current Acc: 91.4%\n",
      "[BATCH 140] Avg Loss: 0.2069, Current Acc: 91.6%\n",
      "[BATCH 150] Avg Loss: 0.2055, Current Acc: 91.7%\n",
      "[BATCH 160] Avg Loss: 0.2088, Current Acc: 91.5%\n",
      "[BATCH 170] Avg Loss: 0.2135, Current Acc: 91.2%\n",
      "[BATCH 180] Avg Loss: 0.2143, Current Acc: 91.2%\n",
      "[BATCH 190] Avg Loss: 0.2140, Current Acc: 91.3%\n",
      "[BATCH 200] Avg Loss: 0.2120, Current Acc: 91.4%\n",
      "[BATCH 210] Avg Loss: 0.2074, Current Acc: 91.6%\n",
      "[BATCH 220] Avg Loss: 0.2093, Current Acc: 91.6%\n",
      "[BATCH 230] Avg Loss: 0.2070, Current Acc: 91.6%\n",
      "[BATCH 240] Avg Loss: 0.2038, Current Acc: 91.8%\n",
      "[BATCH 250] Avg Loss: 0.2005, Current Acc: 91.8%\n",
      "[BATCH 260] Avg Loss: 0.1979, Current Acc: 92.0%\n",
      "[BATCH 270] Avg Loss: 0.1988, Current Acc: 91.9%\n",
      "[BATCH 280] Avg Loss: 0.1975, Current Acc: 92.0%\n",
      "[BATCH 290] Avg Loss: 0.1957, Current Acc: 92.1%\n",
      "[BATCH 300] Avg Loss: 0.1938, Current Acc: 92.2%\n",
      "[BATCH 310] Avg Loss: 0.1912, Current Acc: 92.4%\n",
      "[BATCH 320] Avg Loss: 0.1891, Current Acc: 92.4%\n",
      "[BATCH 330] Avg Loss: 0.1870, Current Acc: 92.5%\n",
      "[BATCH 340] Avg Loss: 0.1868, Current Acc: 92.5%\n",
      "[BATCH 350] Avg Loss: 0.1863, Current Acc: 92.6%\n",
      "[BATCH 360] Avg Loss: 0.1878, Current Acc: 92.5%\n",
      "[BATCH 370] Avg Loss: 0.1884, Current Acc: 92.4%\n",
      "[BATCH 380] Avg Loss: 0.1893, Current Acc: 92.4%\n",
      "[BATCH 390] Avg Loss: 0.1888, Current Acc: 92.4%\n",
      "[BATCH 400] Avg Loss: 0.1868, Current Acc: 92.5%\n",
      "[BATCH 410] Avg Loss: 0.1854, Current Acc: 92.6%\n",
      "[BATCH 420] Avg Loss: 0.1852, Current Acc: 92.5%\n",
      "[BATCH 430] Avg Loss: 0.1850, Current Acc: 92.6%\n",
      "[BATCH 440] Avg Loss: 0.1838, Current Acc: 92.6%\n",
      "[BATCH 450] Avg Loss: 0.1871, Current Acc: 92.5%\n",
      "[BATCH 460] Avg Loss: 0.1897, Current Acc: 92.4%\n",
      "[BATCH 470] Avg Loss: 0.1904, Current Acc: 92.3%\n",
      "[BATCH 480] Avg Loss: 0.1897, Current Acc: 92.4%\n",
      "[BATCH 490] Avg Loss: 0.1900, Current Acc: 92.4%\n",
      "[BATCH 500] Avg Loss: 0.1888, Current Acc: 92.5%\n",
      "[BATCH 510] Avg Loss: 0.1884, Current Acc: 92.5%\n",
      "[BATCH 520] Avg Loss: 0.1875, Current Acc: 92.5%\n",
      "[BATCH 530] Avg Loss: 0.1870, Current Acc: 92.5%\n",
      "[BATCH 540] Avg Loss: 0.1859, Current Acc: 92.6%\n",
      "\n",
      "[EPOCH 4 SUMMARY]\n",
      "  Loss: 0.1852\n",
      "  Accuracy: 92.58%\n",
      "  Time: 920.4 seconds\n",
      "[VALIDATION] Loss: 0.2364, Accuracy: 92.53%\n",
      "\n",
      "[EPOCH 5/5] Starting...\n",
      "[BATCH 1] Loss: 0.2885, Acc: 90.6%, Time: 0.0s\n",
      "[INFO] First batch completed! Training continues...\n",
      "[BATCH 10] Avg Loss: 0.1341, Current Acc: 93.1%\n",
      "[BATCH 20] Avg Loss: 0.1260, Current Acc: 94.4%\n",
      "[BATCH 30] Avg Loss: 0.1329, Current Acc: 94.1%\n",
      "[BATCH 40] Avg Loss: 0.1219, Current Acc: 94.7%\n",
      "[BATCH 50] Avg Loss: 0.1154, Current Acc: 95.1%\n",
      "[BATCH 60] Avg Loss: 0.1142, Current Acc: 95.1%\n",
      "[BATCH 70] Avg Loss: 0.1097, Current Acc: 95.3%\n",
      "[BATCH 80] Avg Loss: 0.1082, Current Acc: 95.5%\n",
      "[BATCH 90] Avg Loss: 0.1042, Current Acc: 95.7%\n",
      "[BATCH 100] Avg Loss: 0.1058, Current Acc: 95.6%\n",
      "[BATCH 110] Avg Loss: 0.1093, Current Acc: 95.6%\n",
      "[BATCH 120] Avg Loss: 0.1065, Current Acc: 95.8%\n",
      "[BATCH 130] Avg Loss: 0.1058, Current Acc: 95.8%\n",
      "[BATCH 140] Avg Loss: 0.1042, Current Acc: 95.8%\n",
      "[BATCH 150] Avg Loss: 0.1040, Current Acc: 95.9%\n",
      "[BATCH 160] Avg Loss: 0.1048, Current Acc: 95.8%\n",
      "[BATCH 170] Avg Loss: 0.1035, Current Acc: 95.9%\n",
      "[BATCH 180] Avg Loss: 0.1058, Current Acc: 95.8%\n",
      "[BATCH 190] Avg Loss: 0.1055, Current Acc: 95.8%\n",
      "[BATCH 200] Avg Loss: 0.1057, Current Acc: 95.8%\n",
      "[BATCH 210] Avg Loss: 0.1093, Current Acc: 95.7%\n",
      "[BATCH 220] Avg Loss: 0.1107, Current Acc: 95.7%\n",
      "[BATCH 230] Avg Loss: 0.1104, Current Acc: 95.7%\n",
      "[BATCH 240] Avg Loss: 0.1141, Current Acc: 95.5%\n",
      "[BATCH 250] Avg Loss: 0.1129, Current Acc: 95.6%\n",
      "[BATCH 260] Avg Loss: 0.1129, Current Acc: 95.6%\n",
      "[BATCH 270] Avg Loss: 0.1136, Current Acc: 95.5%\n",
      "[BATCH 280] Avg Loss: 0.1141, Current Acc: 95.5%\n",
      "[BATCH 290] Avg Loss: 0.1138, Current Acc: 95.5%\n",
      "[BATCH 300] Avg Loss: 0.1129, Current Acc: 95.6%\n",
      "[BATCH 310] Avg Loss: 0.1112, Current Acc: 95.6%\n",
      "[BATCH 320] Avg Loss: 0.1114, Current Acc: 95.7%\n",
      "[BATCH 330] Avg Loss: 0.1097, Current Acc: 95.7%\n",
      "[BATCH 340] Avg Loss: 0.1082, Current Acc: 95.8%\n",
      "[BATCH 350] Avg Loss: 0.1071, Current Acc: 95.8%\n",
      "[BATCH 360] Avg Loss: 0.1064, Current Acc: 95.9%\n",
      "[BATCH 370] Avg Loss: 0.1060, Current Acc: 95.9%\n",
      "[BATCH 380] Avg Loss: 0.1056, Current Acc: 95.9%\n",
      "[BATCH 390] Avg Loss: 0.1058, Current Acc: 95.9%\n",
      "[BATCH 400] Avg Loss: 0.1061, Current Acc: 95.9%\n",
      "[BATCH 410] Avg Loss: 0.1068, Current Acc: 95.9%\n",
      "[BATCH 420] Avg Loss: 0.1073, Current Acc: 95.8%\n",
      "[BATCH 430] Avg Loss: 0.1075, Current Acc: 95.8%\n",
      "[BATCH 440] Avg Loss: 0.1072, Current Acc: 95.8%\n",
      "[BATCH 450] Avg Loss: 0.1096, Current Acc: 95.8%\n",
      "[BATCH 460] Avg Loss: 0.1119, Current Acc: 95.7%\n",
      "[BATCH 470] Avg Loss: 0.1132, Current Acc: 95.6%\n",
      "[BATCH 480] Avg Loss: 0.1139, Current Acc: 95.6%\n",
      "[BATCH 490] Avg Loss: 0.1137, Current Acc: 95.6%\n",
      "[BATCH 500] Avg Loss: 0.1138, Current Acc: 95.6%\n",
      "[BATCH 510] Avg Loss: 0.1128, Current Acc: 95.7%\n",
      "[BATCH 520] Avg Loss: 0.1130, Current Acc: 95.7%\n",
      "[BATCH 530] Avg Loss: 0.1129, Current Acc: 95.7%\n",
      "[BATCH 540] Avg Loss: 0.1124, Current Acc: 95.7%\n",
      "\n",
      "[EPOCH 5 SUMMARY]\n",
      "  Loss: 0.1121\n",
      "  Accuracy: 95.74%\n",
      "  Time: 813.4 seconds\n",
      "[VALIDATION] Loss: 0.2738, Accuracy: 91.33%\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE!\n",
      "Model saved as 'cancer_cnn_model.pth'\n",
      "Final Training Accuracy: 95.74%\n",
      "Final Validation Accuracy: 91.33%\n",
      "Total training time: 5292.3 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CancerCNN(nn.Module):\n",
    "   \n",
    "    def __init__(self, num_classes=5, input_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extractor \n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        \n",
    "        flat_dim = self._get_flatten_dim(input_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(flat_dim, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 20)\n",
    "        self.fc4 = nn.Linear(20, num_classes)\n",
    "\n",
    "    def _get_flatten_dim(self, input_size: int) -> int:\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 3, input_size, input_size)   # batch=1\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            return x.numel()  # total features per sample\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # [B, features]\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        # IMPORTANT: return raw logits for CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['colon_aca', 'colon_n', 'lung_aca', 'lung_n', 'lung_scc']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        \n",
    "        print(f\"Root directory: {root_dir}\")\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff'))]\n",
    "                for img_name in images[:5000]:  # Used to check if loading works on smaller samples\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "                print(f\"  {class_name}: {len(images)} images\")\n",
    "        \n",
    "        print(f\"Total samples loaded: {len(self.samples)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# 3. TRAINING FUNCTION \n",
    "def train_with_immediate_output():\n",
    "      \n",
    "    print(\"Traisn MODEL\")\n",
    "     \n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n[SETUP] Device: {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        print(f\"[SETUP] GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"[SETUP] CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Fixed size\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\n[DATA] Loading dataset...\")\n",
    "    data_dir = \"lungcolon\"\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"ERROR:not found at {data_dir}\")\n",
    "\n",
    "        return\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = CancerDataset(root_dir=data_dir, transform=transform)\n",
    "    \n",
    "    # Split dataset (70% train, 15% val, 15% test)\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.15 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset, [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[DATA] Dataset splits:\")\n",
    "    print(f\"  Training: {len(train_dataset)} samples\")\n",
    "    print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "    print(f\"  Test: {len(test_dataset)} samples\")\n",
    "    print(f\"  Classes: {dataset.classes}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)  # num_workers=0 for immediate output\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create model\n",
    "    model = CancerCNN(num_classes=len(dataset.classes), input_size=128).to(device)\n",
    "    \n",
    "      \n",
    "    print(\"MODEL ARCHITECTURE\")\n",
    "     \n",
    "    print(model)\n",
    "    \n",
    "    # Calculate parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\n[MODEL] Total parameters: {total_params:,}\")\n",
    "    print(f\"[MODEL] Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.004)\n",
    "    \n",
    "      \n",
    "    print(\"TRAINING\")\n",
    "     \n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 5  # Start with 3 epochs for quick results IF CPU it will take hors \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        print(f\"\\n[EPOCH {epoch+1}/{num_epochs}] Starting...\")\n",
    "    \n",
    "        \n",
    "        # Training batches\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Move to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_total = labels.size(0)\n",
    "            batch_correct = (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item()\n",
    "            total += batch_total\n",
    "            correct += batch_correct\n",
    "            \n",
    "            # Print immediate progress\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                print(f\"[BATCH 1] Loss: {loss.item():.4f}, Acc: {100*batch_correct/batch_total:.1f}%, Time: {batch_time:.1f}s\")\n",
    "                print(f\"[INFO] First batch completed! Training continues...\")\n",
    "            elif (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n",
    "                avg_loss = running_loss / (batch_idx + 1)\n",
    "                current_acc = 100 * correct / total\n",
    "                print(f\"[BATCH {batch_idx+1}] Avg Loss: {avg_loss:.4f}, Current Acc: {current_acc:.1f}%\")\n",
    "        \n",
    "        # Epoch statistics\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        print(f\"\\n[EPOCH {epoch+1} SUMMARY]\")\n",
    "        print(f\"  Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {epoch_acc:.2f}%\")\n",
    "        print(f\"  Time: {epoch_time:.1f} seconds\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"[VALIDATION] Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "        \n",
    "        \n",
    "        model.train()  # Set back to training mode\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'epochs': num_epochs,\n",
    "        'loss': epoch_loss,\n",
    "        'accuracy': epoch_acc,\n",
    "        'classes': dataset.classes\n",
    "    }, 'cancer_cnn_model.pth')\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "     \n",
    "    print(f\"Model saved as 'cancer_cnn_model.pth'\")\n",
    "    print(f\"Final Training Accuracy: {epoch_acc:.2f}%\")\n",
    "    print(f\"Final Validation Accuracy: {val_acc:.2f}%\")\n",
    "    print(f\"Total training time: {time.time() - start_time:.1f} seconds\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# 5. MAIN \n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "      \n",
    "    print(\"CANCER HISTOPATHOLOGY CNN CLASSIFIER\")\n",
    "     \n",
    "    \n",
    "    # Step 1: Quick test\n",
    "    print(\"\\n Running quick CNN connection test...\")\n",
    "    #quick_test()\n",
    "    \n",
    "    # Step 2: Training\n",
    "    print(\"\\n Starting actual training...\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        model = train_with_immediate_output()\n",
    "        \n",
    "          \n",
    "        \n",
    "    except Exception as e:\n",
    "       print (f\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242692f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
