{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0804a25",
   "metadata": {},
   "source": [
    "# Softmax layer implementation (with cross-entropy loss)\n",
    "If we have network trained to predict among 10 input classes like cifar-10, softmax activation layer is used in the output layer. It calculates the probability of each class from which the input belong. Number of softmax units in output layer should always equal to number of class in dataset. So, each unit can hold probability of a sample/input being of that class. So, sum of probability of all softmax units always equals 1 (probability distribution).\n",
    "\n",
    "### Things to remember\n",
    "- This function converts real value into a probability.\n",
    "- It is only used at the output layer.\n",
    "- Higher probability can be considered as actual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "212d06e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        self.probs = None \n",
    "        self.cache_labels = None # Cache store output for backward pass.\n",
    "\n",
    "    def forward(self, logits: np.ndarray, cache_labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Performs forward pass for this softmax layer\n",
    "\n",
    "        Args:\n",
    "        - logits: shape (batch_size, num_classes). For reference it's just x but \n",
    "                When a single sample  x that has gone through trenches with couple of hidden layers and \n",
    "                when it passes through linear layer and reaches SoftMax it's called \"logits\".\n",
    "        - labels: one-hot encoded, shape (batch_size, num_classes)\n",
    "                Remember it has to be one-hot encoded\n",
    "        \n",
    "        Returns:\n",
    "        - Int scalar loss value\n",
    "        \"\"\"\n",
    "        # Softmax\n",
    "        shifted = logits - np.max(logits, axis=1, keepdims=True)  # stability\n",
    "        exp_x = np.exp(shifted)\n",
    "        self.probs = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "        # Cache labels\n",
    "        self.cache_labels = cache_labels\n",
    "\n",
    "        # Cross-entropy loss\n",
    "        batch_size = logits.shape[0]\n",
    "        log_likelihood = -np.log(np.sum(self.probs * cache_labels, axis=1))\n",
    "        loss = np.sum(log_likelihood) / batch_size\n",
    "        return loss\n",
    "\n",
    "    '''\n",
    "    Why jacobian-vector product is not suitable over this?\n",
    "    Jacobian-vector product is correct in theory, but inefficient in practice.\n",
    "    With SoftMax + cross-entropy, the gradient simplifies to probs - labels.\n",
    "    That's why your SoftmaxCrossEntropy.backward() is leaner and better suited than a full Jacobian approach.\n",
    "\n",
    "    '''\n",
    "    def backward(self) -> np.ndarray:\n",
    "        batch_size = self.cache_labels.shape[0]\n",
    "        dX = (self.probs - self.cache_labels) / batch_size\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486667b",
   "metadata": {},
   "source": [
    "# Testing Softmax implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ef3e3",
   "metadata": {},
   "source": [
    "### Forward Pass (Test)\n",
    "Loading the dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d39c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8be74",
   "metadata": {},
   "source": [
    "Function to plot a sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fbd1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "def plot_data(X, y, index):\n",
    "    plt.figure(figsize=(10, 2))  # Scaling the image\n",
    "    plt.imshow(X[index])\n",
    "    plt.xlabel(classes[(y[index])[0]]) # Because y_train[index] return [val]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d1b0b",
   "metadata": {},
   "source": [
    "Pipeline to build\n",
    "- Image (32x32x3 -> 3072 features) -> hidden layers (eg. ... -> Linear layer(mostly before softmax in usual design)) -> softmax + loss entropy -> probabilities\n",
    "\n",
    "Current pipeline (for testing)\n",
    "- Image -> softmax -> probabiltity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4df2aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 7\n",
      "Loss: 41.66676707127592\n",
      "Gradient: [[ 2.66660736e-13  4.32930938e-23  8.07802945e-13  1.05902255e-08\n",
      "   5.21080679e-17  1.58621037e-01 -1.00000000e+00  8.41373308e-01\n",
      "   4.39864035e-06  1.24555925e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Vector of single image.\n",
    "img = X_train[0]\n",
    "reshaped_img = img.reshape(1, -1) # Because current softmax expects in form (1, 3072) not (3072) i.e. 32x32x3\n",
    "\n",
    "softmax = SoftmaxCrossEntropy()\n",
    "\n",
    "# Synthetic weights + bias\n",
    "np.random.seed(0)\n",
    "W = np.random.rand(3072, 10) * 0.01\n",
    "b = np.zeros(10)\n",
    "\n",
    "# Applying linear layer\n",
    "logits = np.dot(reshaped_img, W) + b  \n",
    "\n",
    "# One-hot label for frog.\n",
    "# classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "label_index = y_train[0][0] # For frog i.e. 6 cause 6th index in class.\n",
    "labels = np.zeros((1, 10))\n",
    "labels[0, label_index] = 1\n",
    "\n",
    "# Forward pass\n",
    "loss = softmax.forward(logits, labels)\n",
    "print(\"Predicted class:\", np.argmax(softmax.probs))\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Backward pass\n",
    "grad = softmax.backward()\n",
    "print(\"Gradient:\", grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1612934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "b = np.random.rand(10)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IN3063_Coursework_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
